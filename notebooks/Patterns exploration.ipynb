{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns processing\n",
    "This notebook reads in the patterns data and:\n",
    "    1. Filters down to Philadelphia zipcodes based on a (currently hard-coded) list.\n",
    "        a. I have not yet run this using this list. Previously it filtered down by looking at the state and city columns in the data\n",
    "    3. Concatenates the files\n",
    "    4. Writes the result to philly_patterns.csv in the processed data folder.\n",
    "    \n",
    "It takes a long time to run and the resulting dataset is very large so it is worth thinking about ways to cut down the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from safegraph_py_functions import safegraph_py_functions as sgpy\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "root_dir = os.environ.get(\"ROOT_DIR\")\n",
    "raw_data_dir = os.path.join(root_dir,'data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory where we want to put all the data\n",
    "patterns_path = os.path.join(raw_data_dir,'monthly-patterns')\n",
    "# print(local)\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(patterns_path):\n",
    "    for file in f:\n",
    "        if file.endswith('.csv.gz') and 'patterns-part' in file:\n",
    "            files.append(os.path.join(patterns_path, r, file))\n",
    "\n",
    "norm_files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(patterns_path):\n",
    "    for file in f:\n",
    "        if 'normalization_stats.csv' in file:\n",
    "            norm_files.append(os.path.join(patterns_path, r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = os.path.join(root_dir,'data/processed')\n",
    "philly_places = pd.read_csv(os.path.join(processed_data_dir,'philly_places.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['safegraph_place_id', 'location_name', 'street_address',\n",
    "       'city', 'region', 'postal_code', 'safegraph_brand_ids', 'brands',\n",
    "       'date_range_start', 'date_range_end', 'raw_visit_counts',\n",
    "       'raw_visitor_counts', 'visits_by_day', 'poi_cbg', 'visitor_home_cbgs',\n",
    "       'visitor_daytime_cbgs', 'visitor_work_cbgs',\n",
    "       'distance_from_home', 'median_dwell',\n",
    "       'device_type']\n",
    "\n",
    "# for files with information disaggregated at the state level, keep only the country-wide info\n",
    "def keep_total_level(norm_stats):\n",
    "    if 'region' in norm_stats.columns:\n",
    "        if len(norm_stats[norm_stats['region'] == 'ALL_STATES']) == 0:\n",
    "            raise ValueError('no region named \"ALL_STATES\"')\n",
    "        norm_stats = norm_stats[norm_stats['region'] == 'ALL_STATES']\n",
    "        norm_stats = norm_stats.drop(columns = ['region'])\n",
    "    return norm_stats\n",
    "\n",
    "def filter_to_philly(file):\n",
    "    # zip codes are read as integers rather than strings so we add leading zeros.\n",
    "    # this is not strictly necessary since Philadelphia zipcodes don't have leading zeros.\n",
    "    \n",
    "    # Philadelphia selection\n",
    "    # HK: adding leading zeros because some zipcodes in MA are 0191X.\n",
    "    df = pd.read_csv(file)\n",
    "    df['postal_code'] = df['postal_code'].apply(lambda x: ('00000'+str(x))[-5:])\n",
    "    in_philly = df['postal_code'].astype(str).str.startswith(\"191\")\n",
    "    df = df.loc[in_philly]\n",
    "    df = df[keep_cols]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_places(df):\n",
    "    df = df.reset_index(drop = True)\n",
    "    df['date_range_start'] = pd.to_datetime(\n",
    "        df['date_range_start'].apply(lambda x: x[:10])\n",
    "    )\n",
    "    if len(df['date_range_start'].unique()) > 1:\n",
    "        print('More than one date in {0}!'.format(file))\n",
    "    file_date = df.loc[0,'date_range_start']\n",
    "    current = (philly_places['valid_from'] <= file_date) & (philly_places['valid_to'] > file_date)\n",
    "    current_places = philly_places.loc[current, ['safegraph_place_id','top_category']]\n",
    "    df = df.merge(current_places, how = 'left')\n",
    "    return df\n",
    "\n",
    "def get_norm_stats(df):\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    return df\n",
    "    \n",
    "def explode(df):\n",
    "    # The visits_by_day column contains a list of integers. \n",
    "    # This explodes that list so we get one row per day.\n",
    "    df = sgpy.explode_json_array(\n",
    "        df, array_column ='visits_by_day', value_col_name='day_visit_counts', \n",
    "        place_key='safegraph_place_id', file_key='date_range_start', array_sequence='day', \n",
    "        keep_index=False, zero_index=False)\n",
    "    df['date_range_start'] = pd.to_datetime(df['date_range_start'])\n",
    "    temp = df['day'].apply(lambda x: pd.Timedelta(x-1, unit='D'))\n",
    "    df['date'] = df['date_range_start'] + temp\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_stats = pd.concat([keep_total_level(pd.read_csv(file)) for file in norm_files])\n",
    "norm_stats['year'] = norm_stats['year'].astype(int)\n",
    "norm_stats['month'] = norm_stats['month'].astype(int)\n",
    "norm_stats['day'] = norm_stats['day'].astype(int)\n",
    "# HK: I only downloaded patterns data from 2019 onwards due to memory constraints\n",
    "norm_stats = norm_stats[norm_stats['year'] >= 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "philly_patterns = [filter_to_philly(file) for file in files]\n",
    "philly_patterns = pd.concat(philly_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = os.path.join(root_dir,'data/processed')\n",
    "philly_patterns.to_csv(\n",
    "    os.path.join(processed_data_dir,\"philly_patterns.csv.tar.gz\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "philly_patterns_df.to_csv(os.path.join(processed_data_dir,'philly_patterns.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
