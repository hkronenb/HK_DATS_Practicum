{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns processing\n",
    "This notebook reads in the patterns data and:\n",
    "    1. Filters down to Philadelphia zipcodes based on a (currently hard-coded) list.\n",
    "        a. I have not yet run this using this list. Previously it filtered down by looking at the state and city columns in the data\n",
    "    3. Concatenates the files\n",
    "    4. Writes the result to philly_patterns.csv in the processed data folder.\n",
    "    \n",
    "It takes a long time to run and the resulting dataset is very large so it is worth thinking about ways to cut down the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_LEVEL = False\n",
    "DAY_LEVEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from safegraph_py_functions import safegraph_py_functions as sgpy\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "ROOT_DIR = os.environ.get(\"ROOT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(ROOT_DIR)\n",
    "from src import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = DATA_DIR / 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all patterns files in the monthly-patterns folder\n",
    "\n",
    "patterns_path = raw_data_dir / \"monthly-patterns\"\n",
    "\n",
    "files = [f for f in patterns_path.glob(\"**/*.csv.gz\")]\n",
    "    \n",
    "norm_files = [f for f in patterns_path.glob(\"**/normalization_stats.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "philly_places = pd.read_csv(DATA_DIR / 'processed' / 'philly_places.csv.tar.gz', low_memory = False)\n",
    "for col in ['valid_from', 'valid_to']:\n",
    "    philly_places[col] = pd.to_datetime(philly_places[col], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['safegraph_place_id', 'location_name', 'street_address',\n",
    "       'city', 'region', 'postal_code', 'safegraph_brand_ids', 'brands',\n",
    "       'date_range_start', 'date_range_end', 'raw_visit_counts',\n",
    "       'raw_visitor_counts', 'visits_by_day', 'poi_cbg', 'visitor_home_cbgs',\n",
    "       'visitor_daytime_cbgs', 'visitor_work_cbgs',\n",
    "       'distance_from_home', 'median_dwell',\n",
    "       'device_type']\n",
    "\n",
    "# for files with information disaggregated at the state level, keep only the country-wide info\n",
    "def keep_total_level(norm_stats):\n",
    "    if 'region' in norm_stats.columns:\n",
    "        if len(norm_stats[norm_stats['region'] == 'ALL_STATES']) == 0:\n",
    "            raise ValueError('no region named \"ALL_STATES\"')\n",
    "        norm_stats = norm_stats[norm_stats['region'] == 'ALL_STATES']\n",
    "        norm_stats = norm_stats.drop(columns = ['region'])\n",
    "    return norm_stats\n",
    "\n",
    "def filter_to_philly(file):\n",
    "    # zip codes are read as integers rather than strings so we add leading zeros.\n",
    "    # this is not strictly necessary since Philadelphia zipcodes don't have leading zeros.\n",
    "    \n",
    "    # Philadelphia selection\n",
    "    # HK: adding leading zeros because some zipcodes in MA are 0191X.\n",
    "    df = pd.read_csv(file)\n",
    "    df['postal_code'] = df['postal_code'].apply(lambda x: ('00000'+str(x))[-5:])\n",
    "    in_philly = df['postal_code'].astype(str).str.startswith(\"191\")\n",
    "    df = df.loc[in_philly]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_places(df):\n",
    "    df = df.reset_index(drop = True)\n",
    "    df['date_range_start'] = pd.to_datetime(\n",
    "        df['date_range_start'].apply(lambda x: x[:10])\n",
    "    )\n",
    "    if len(df['date_range_start'].unique()) > 1:\n",
    "        print('More than one date in {0}!'.format(file))\n",
    "    file_date = df.loc[0,'date_range_start']\n",
    "    current = (philly_places['valid_from'] <= file_date) & (philly_places['valid_to'] > file_date)\n",
    "    current_places = philly_places[current]\n",
    "    df = df.merge(current_places, on = 'safegraph_place_id', how = 'left')\n",
    "    return df\n",
    "\n",
    "def get_norm_stats(df, norm_df):\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df = df.merge(norm_df, on = ['year','month','day'])\n",
    "    return df\n",
    "    \n",
    "def explode(df):\n",
    "    # The visits_by_day column contains a list of integers. \n",
    "    # This explodes that list so we get one row per day.\n",
    "    df = sgpy.explode_json_array(\n",
    "        df, array_column ='visits_by_day', value_col_name='day_visit_counts', \n",
    "        place_key='safegraph_place_id', file_key='date_range_start', array_sequence='day', \n",
    "        keep_index=False, zero_index=False)\n",
    "    df['date_range_start'] = pd.to_datetime(df['date_range_start'])\n",
    "    temp = df['day'].apply(lambda x: pd.Timedelta(x-1, unit='D'))\n",
    "    df['date'] = df['date_range_start'] + temp\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_stats = pd.concat([keep_total_level(pd.read_csv(file)) for file in norm_files])\n",
    "norm_stats['year'] = norm_stats['year'].astype(int)\n",
    "norm_stats['month'] = norm_stats['month'].astype(int)\n",
    "norm_stats['day'] = norm_stats['day'].astype(int)\n",
    "# HK: I only downloaded patterns data from 2019 onwards due to memory constraints\n",
    "norm_stats = norm_stats[norm_stats['year'] >= 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MONTH_LEVEL or DAY_LEVEL:\n",
    "    philly_patterns = []\n",
    "    for file in files:\n",
    "        df = filter_to_philly(file) \n",
    "        df = get_places(df)\n",
    "        philly_patterns.append(df)\n",
    "if MONTH_LEVEL:\n",
    "    pd.concat(philly_patterns).to_csv(\n",
    "        DATA_DIR / 'processed' / \"philly_patterns.csv.tar.gz\", index=False\n",
    "    )\n",
    "if DAY_LEVEL:\n",
    "    philly_patterns = [get_norm_stats(explode(df), norm_stats) for df in philly_patterns]\n",
    "    pd.concat(philly_patterns).to_csv(\n",
    "        DATA_DIR / 'processed' / \"philly_patterns_exploded.csv.tar.gz\", index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
